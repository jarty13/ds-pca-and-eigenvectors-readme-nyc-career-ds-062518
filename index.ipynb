{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis, Eigenvectors, & Eigenvalues: lecture\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Up until now, We have focused on supervised learning. This group of methods aims at predicting labels based on training data that is labeled as well. Principal Component Analysis is our first so-called \"unsupervised\" estimator. Generally, the aim of unsupervised estimators is to reveal interesting data patterns without having any reference labels.\n",
    "\n",
    "The first unsupervised learning algorithm is Principal Component Analysis, also referred to as PCA. PCA is a dimensionality reduction technique which is often used in practice for visualization, feature extraction, noise filtering, etc.\n",
    "\n",
    "Generally, PCA would be applied on data sets with many variables. PCA creates new variables that are linear combinations of the original variables. The idea is to reduce the dimension of the data considerably while maintaining as much information as possible. While the purpose is to significantly reduce the dimensionality, the maximum amount of new variables that can possibly be created is equal to the number of original variables. A nice feature of PCA is that the newly created variables are uncorrelated.\n",
    "\n",
    "**A simple example** : Imagine that a data set consists of the height and weight of a group of people. One could imagine that these 2 metrics are heavily correlated, so we could basically summarize these 2 metrics in one variable, a linear combination of the two. This one variable will contain most of the information from 2 variables in one. It is important to note that the effectiveness of PCA strongly depends on the structure of the correlation matrix of the existing variables!\n",
    "\n",
    "## 2. A Brief Aside: Eigenvalues and Eigenvectors\n",
    "\n",
    "In order to understand how PCA actually works, we first need to be comfortable with **_Eigenvectors_** and **_Eigenvalues_**.\n",
    "\n",
    "An eigenvector is a vector that after transformation hasn't changed, except by a scalar value known as the *eigenvalue*.\n",
    "\n",
    "### 2.1 Definition\n",
    "\n",
    "If there exists a square matrix $A$ (an n x n matrix) , then a scalar $\\lambda$ is called the **eigenvalue** of $A$ if there is a non-zero vector $v$ such that\n",
    "\n",
    "$$Av = \\lambda v$$.\n",
    "\n",
    "This vector $v$ is then called the **eigenvector** of A corresponding to $\\lambda$.\n",
    "\n",
    "Eigenvalues and eigenvectors are very useful and have tons of applications!\n",
    "\n",
    "\n",
    "Imagine you have a matrix\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}\n",
    "   3 & 2 \\\\\n",
    "   3 & -2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have an eigenvector\n",
    "\\begin{equation}\n",
    "v = \\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Let's perform the multiplication $A v$\n",
    "\n",
    "\\begin{equation}\n",
    "Av  = \\begin{bmatrix}\n",
    "   3 & 2 \\\\\n",
    "   3 & -2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    3*2+2*1 \\\\\n",
    "   3*2+(-2*1)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    8 \\\\\n",
    "   4\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now we want to see if we can find a $\\lambda$ such that\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Av = \\begin{bmatrix}\n",
    "   8 \\\\\n",
    "   4\n",
    "\\end{bmatrix}= \\lambda \\begin{bmatrix}\n",
    "   2 \\\\\n",
    "   1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Turns out $\\lambda =4$ is the eigenvalue for our proposed eigenvector!\n",
    "\n",
    "\n",
    "### 2.2 But how can you find values of eigenmatrices?\n",
    "\n",
    "An $n xn$ matrix has n eigenvectors and n eigenvalues! How to find the eigenvalues?\n",
    "\n",
    "$ det(A- \\lambda I)= 0$\n",
    "\n",
    "\\begin{equation}\n",
    "det(A- \\lambda I) = det\\begin{bmatrix}\n",
    "   3-\\lambda & 2 \\\\\n",
    "   3 & -2-\\lambda\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This way we indeed find that 4 is an eigenvalue, and so is -3! You'll learn about the connection between eigenvalues and eigenmatrices in a second.\n",
    "\n",
    "https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/\n",
    "\n",
    "https://www.youtube.com/watch?v=ue3yoeZvt8E\n",
    "\n",
    "\n",
    "## 3.  PCA: some notation\n",
    "\n",
    "### 3.1 The data matrix\n",
    "\n",
    "Let's say we have P variables $X_1, X_2, \\dots, X_p$ and $n$ observations $1,...,n$. Or data looks like this:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    X_{11} & X_{12} & X_{13} & \\dots  & X_{1p} \\\\\n",
    "    X_{21} & X_{22} & X_{23} & \\dots  & X_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    X_{n1} & X_{n2} & X_{n3} & \\dots  & X_{np}\n",
    "\\end{bmatrix}\n",
    "\n",
    "For 2 variables, this is what our data could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a5d37e3e4321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# %matplotlib inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "# %matplotlib inline  \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "X = np.random.normal(2, 1.5, 50)\n",
    "Y = np.random.normal(3, 0.6, 50)\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(-7,7)\n",
    "\n",
    "plt.scatter(X, Y, s = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 The mean & mean-corrected data\n",
    "The mean of the $j$-th variable:\n",
    "\n",
    "$\\bar{X}j = \\dfrac{\\sum{i=1}^n X_{ij}}{n}= \\bar X_{.j}$\n",
    "\n",
    "To get to the mean-corrected data: substract the mean from each $X_{ij}$\n",
    "\n",
    "$x_{ij} = X_{ij}-\\bar X_{.j}$\n",
    "\n",
    "Going back to our two variables example, this is how the data would be shifted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2c34ca93adf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxhline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "X_mean = X- np.mean(X)\n",
    "Y_mean = Y- np.mean(Y)\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(-7,7)\n",
    "plt.scatter(X_mean, Y_mean, s = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 The variance & standardized data\n",
    "$s^2_j = \\dfrac{\\sum_{i=1}^n (X_{ij}-\\bar X_{.j})^2}{n-1}= \\dfrac{\\sum_{i=1}^n x_{ij}^2}{n-1}$\n",
    "\n",
    "To get to the standardized data: divide the mean-corrected data by the standard deviation $s_j$.\n",
    "\n",
    "$z_{ij} = \\dfrac{x_{ij}}{s_{j}}$\n",
    "\n",
    "Going back to the example with 2 variables, this is what standardized data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x116bbe6a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEXxJREFUeJzt3V2MXPV9xvHn2dduWWxCvAjB2l0wCW14S6INbUVb3AAuNAhLuWhplQg1F1aSBpkqEeFFvU+bKhgpyYUF9CZIgAJ5UeSWFyWm4gLK2tiAcRxhlATbQSygrlll4/Hu/nqxu854mdl5OWfmzPzn+5Ei2N05Z36LJs/57f/lHEeEAADp6Cu6AABAvgh2AEgMwQ4AiSHYASAxBDsAJIZgB4DEEOwAkBiCHQASQ7ADQGIGinjTDRs2xMTERBFvDVR1+PBhSdKll15acCVAZXv37n0nIsZqva6QYJ+YmNDU1FQRbw1UtWXLFknSnj17Cq0DqMb2r+p5HUMxAJAYgh0AEkOwA0BiCHYASAzBDgCJIdgBIDEEOwAkhmAHgMQQ7ACQGIIdABJDsANAYgh2AEgMwQ4AiSHYASAxBDsAJIZgB4DE5Bbstvttv2T7J3mdEwDQuDw79h2SDuV4PgBAE3IJdtvjkj4j6YE8zgcAaF5eHftOSXdKWszpfACAJmUOdts3S3o7IvbWeN1221O2p6anp7O+LQCgijw69msk3WL7l5IekfRp299b/aKI2BURkxExOTY2lsPbAgAqyRzsEXF3RIxHxISkWyX9NCI+l7kyAEBTWMcOAIkZyPNkEbFH0p48zwkAaAwdOwAkhmAHgMQQ7ACQGIIdABJDsANAYgh2AEgMwQ4AiSHYASAxBDsAJIZgB4DEEOwAkBiCHQASQ7ADQGIIdgBIDMEOAIkh2AEgMQQ7ACSGYAeAxBDsAJAYgh0AEkOwA0BiCHYASAzBDgCJIdgBIDEEOwAkhmAHgMQQ7ACQGIIdABJDsANAYgh2AEgMwQ4AiSHYASAxBDsAJIZgB4DEEOwAkJjMwW57o+2f2T5k+6DtHXkUBgBozkAO55iX9NWI2Gf7bEl7bT8dEa/lcG4AQIMyd+wR8ZuI2Lf87+9LOiTpwqznBQA0J9cxdtsTkj4h6YU8zwusZa60oFePzWiutFB0KUBHyGMoRpJke1TS45LuiIgTFX6+XdJ2Sdq0aVNeb4seN1da0Nadz+qd90vacPaQnrrjWo0M9RddFlCoXDp224NaCvWHI+KJSq+JiF0RMRkRk2NjY3m8LaAj07N65/2S5k4t6J33SzoyPVt0SUDh8lgVY0kPSjoUEd/KXhJQv81jo9pw9pBGBvu14ewhbR4bLbokoHB5DMVcI+nzkl6xvX/5e/dExO4czg2saWSoX0/dca2OTM9q89gowzCAcgj2iHhOknOoBWjKyFC/Lr9wfdFlAB2DnacAkBiCHT0hy5JIllOi2+S23BHoVFmWRLKcEt2Ijh2FaVcnnGVJJMsp0Y3o2FGIdnbCK0siV96rkSWRWY4FikKwoxCVOuFWrWzJsiSS5ZToRgzFoBDt3li0siSymWDOcmwlTMai1ejYUYhe7YSZjEU70LGjMHl3wmvplC6ZyVi0Ax07ktdJXTKTsWgHgh3Ja+dEbS29OgSF9mIoBsmrNFFb5NBMO4eg0Jvo2JG81V2ypI4ZmgFagY4dXa28816rCy/vkvOawOyUCVlgNTp2dK3ySdFzRwdlWe/O1u7C85jA7KQJWWA1gh1d68zOOxQhlRYWa06Q5jGB2aoJ2bnSAhOryIxgR9cq77xPzi8oYumJLx8eHdIF60f06rGZqgFZ6eEcixGnh3RqhWorli3yVwDyQrCja6103k8efEt3Pf6yfje/qOGBPv3bZ6/Qtu8+11BAzpUWdODojE7NL2rrzmdrHtOKZYudtCwT3Y3JU3S1kaF+/c1l52ts3bBGBvt13rphDQ/2Nzw5emR6VqfmF7UYUfcxeS9b5MHcyAsdO3JR5NhwpeWMjQ6TbB4b1eBAn07NLxYWqmxeQl4IdmTWCWPD5WPmc6UF3fd3H5ckXXbBUkdd68IzMtSvq8bXa660UOjYNg/mRh4IdmTWSWPDlS4ytS48K6EvSWcND9Apo+sR7Misk25sVW3zUbULT3noHz86o6vG6ZbR/Qh2ZNZJY8PVLjLVLjzlF4JT84vsIkUSCHbkolPGhqtdZKpdeMovBIMDfQzDIAkEO5JT6SJT7cJTfiH48v+sV5/drjKBlmEdO3reSugT6kgFwQ7kgDs9opMwFANk1Anr+IFydOxARtWWWNLFoyh07EBGlZZYNtLFc6te5I1gBzKqtMTy1WMzde3GZRgHrcBQDJCD1Xd6rPdOjXk9pg8oR8eOnpF1yKOR4+vdjdtJt2NAOnIJdts3SrpfUr+kByLiG3mcF8hLliGPudKCDh6f0Y5HX9J7s6fqPr6e3biddDsGpCNzsNvul/QdSTdIOirpRds/jojXsp4byEuzd6BcuSC8feKkSvOLCin3O1h2yu0YkI48xtivlvR6RLwRESVJj0jalsN5gUzKlxs2+3SilQvCyflFSdLwQF/HDJmwnBLV5DEUc6GkN8u+PirpT9c64PDhw9qyZUsObw1Uthhx+hmmgwN9ump8vSxptLQgD/Xrpt0fvH3A/v37JemMz+ZihI6vnKe/T5ecNyoPVz6+nSr9ftwSASvyCPZKn6b4wIvs7ZK2S9Lw8HAObwtUN1daOP0M05Xb8Z41PKCzhhv7yPfZp5+sNDLU3zHhWe33A6R8gv2opI1lX49LOr76RRGxS9IuSZqcnIw9e/bk8NZIVR4rWBqdLF3p1Lvhs8n6997kOhuLPIL9RUkfsX2RpGOSbpX0jzmcFz0qj9BKfbVJ6r8fssk8eRoR85K+IulJSYckPRYRB7OeF70rr007qzcNpSb13w/Ny2VQLiJ2S9qdx7kANu3Uh3vMoBpmW9BxUhhmaHXoMsaOtRDs6EidtmmnkaBuR+g2u+EKvYFgB2poNKgPHp/R2ydO6uT8YstCd2W4avrESa0bGdAF60dyPT+6G3d3BGpoZDJ3rrSgHY++pNL8oizpw6OtmSMYGerXj778FzrnDwd1Ym5e2777HDtQcRrBDtTQyO0IjkzP6r3ZUwpJQwN92vn3H2/Z2PfxmTnNzM1zy198AEMxQA2NTOauXtFz2QWtG/dm9RCqIdiBOtQ7mdvOFT0prB5CaxDsQM7auaKn01YPoTMwxo66cZtYoDvQsaMuvbAhZjFCc6WF03dyBLoVHTvq0s6HLmf9y6CZ4+dKCzpwdEYHj5/Q1p3P8lcJuhodO+rSrhUYWf8yaPT4lR2lvzv1+/ubt3snJ/d8Qd4IdtSlXSswsm6Vb+T48ovAuaODGuzv06mFxbYuHeyFIS60H0MxqFs7bhPb7LNJmzm+/CLw3uwpXXLeqC67YF1bw7WdQ1zoHXTs6ChZ/zLIspnIw0uPvmtnx8wmI7SCIz7weNKWm5ycjKmpqba/L7Ba+fj2TVuvk5Tfo/HqHTtnjB31sr03IiZrvY6OHWiBSmPnkioGeF6bjLhAYAXBjp61Onwdob46HxZcy+qx84PHZ/Qvj+1v2SQpk7Aox+Qpetbq8M1z7frmsVGdOzqo4YE+nTs6KEktnSRlEhblCHb0rNUraJrtcKttiLJ8+p8Xb8i22qeWrKuJkBaGYlCoZsaF8xpLXr2C5qbdjQ/DVBsCOTI9q3dnSzo5v6h3Z0s6PjPX0n0A3OkR5Qh2FKaZceF6j6k3/LNOXFbbEFVpGWOr78TInR6xgmBHYZrZZVrPMa2aSKx0sai2Dp0OGkUi2FGYZjbn1HNM1tsSVFLtYrFWgNNBoygEOwrTTFdbzzGt2M251sWCAEenIdhRqGZCsdYxrRgGYes/ugnBjiTl3UUzZo5uwjp2JKEdj+1rx90tgTzQsaPrsZ0eOBMdO7oe2+mBMxHs6HpspwfOxFAMuh4Tm8CZCHYkgbXkwO8xFAMAiSHYASAxmYLd9jdt/9z2y7Z/YPucvAoDADQna8f+tKTLI+JKSb+QdHf2kgAAWWQK9oh4KiLml798XtJ49pKAD2rHzlIgFXmuivmCpEdzPB8giZ2lQKNqduy2n7H9aoX/bSt7zb2S5iU9vMZ5ttuesj01PT2dT/XoCewsBRpTs2OPiOvX+rnt2yTdLOm6iIg1zrNL0i5JmpycrPo6YDVumQs0JtNQjO0bJX1d0rUR8dt8SgLOxM5SoDFZx9i/LWlY0tO2Jen5iPhi5qqAVdhZCtQvU7BHxCV5FQIAyAc7TwEgMQQ7ACSGYAeAxBDsAJAYgh0AEkOwA0BiCHYASAzBDgCJIdgBIDEEOwAkhmAHgMQQ7ACQGIIdABJDsANAYgh2AEgMwQ4AiSHYASAxBDsAJIZgB4DEEOwAkBiCHQASQ7ADQGIIdgBIDMEOAIkh2AEgMQQ7ACSGYAeAxBDsAJAYgh0AEkOwA0BiCHYASAzBDgCJIdgBIDEEOwAkJpdgt/0122F7Qx7nAwA0L3Ow294o6QZJv85eDgAgqzw69vsk3SkpcjgXACCjTMFu+xZJxyLiQE71AAAyGqj1AtvPSDq/wo/ulXSPpK31vJHt7ZK2S9KmTZsaKBEA0IiawR4R11f6vu0rJF0k6YBtSRqXtM/21RHxVoXz7JK0S5ImJycZtgGAFqkZ7NVExCuSzlv52vYvJU1GxDs51AUAaBLr2AEgMU137KtFxERe5wIANI+OHQASQ7ADQGIIdgBIDMEOAIkh2AEgMQQ7ACSGYAeAxBDsAJAYgh0AEkOwA0BiCHYASAzBDgCJIdgBIDEEOwAkhmAHgMQQ7ACQGIIdABLjiPY/V9r2tKRftfAtNkjq5mevUn9xurl2ifqL1ur6/ygixmq9qJBgbzXbUxExWXQdzaL+4nRz7RL1F61T6mcoBgASQ7ADQGJSDfZdRReQEfUXp5trl6i/aB1Rf5Jj7ADQy1Lt2AGgZyUd7LZvt33Y9kHb/150PY2y/TXbYXtD0bU0wvY3bf/c9su2f2D7nKJrqoftG5c/L6/bvqvoehphe6Ptn9k+tPx531F0TY2y3W/7Jds/KbqWRtk+x/b3lz/3h2z/eZH1JBvstv9a0jZJV0bEZZL+o+CSGmJ7o6QbJP266Fqa8LSkyyPiSkm/kHR3wfXUZLtf0nck3STpY5L+wfbHiq2qIfOSvhoRfyLpzyT9c5fVL0k7JB0quogm3S/pvyPijyVdpYJ/j2SDXdKXJH0jIk5KUkS8XXA9jbpP0p2Sum4SJCKeioj55S+flzReZD11ulrS6xHxRkSUJD2ipcagK0TEbyJi3/K/v6+lYLmw2KrqZ3tc0mckPVB0LY2yvU7SX0l6UJIiohQR/1dkTSkH+0cl/aXtF2w/a/tTRRdUL9u3SDoWEQeKriUHX5D0X0UXUYcLJb1Z9vVRdVEwlrM9IekTkl4otpKG7NRSI7NYdCFNuFjStKT/XB5KesD2WUUWNFDkm2dl+xlJ51f40b1a+t0+pKU/Sz8l6THbF0eHLAOqUfs9kra2t6LGrFV/RPxo+TX3ammI4OF21tYkV/heR3xWGmF7VNLjku6IiBNF11MP2zdLejsi9treUnQ9TRiQ9ElJt0fEC7bvl3SXpH8tsqCuFRHXV/uZ7S9JemI5yP/X9qKW7uMw3a761lKtdttXSLpI0gHb0tIwxj7bV0fEW20scU1r/beXJNu3SbpZ0nWdcjGt4aikjWVfj0s6XlAtTbE9qKVQfzginii6ngZcI+kW238r6Q8krbP9vYj4XMF11euopKMRsfIX0ve1FOyFSXko5oeSPi1Jtj8qaUhdcHOhiHglIs6LiImImNDSh+aTnRTqtdi+UdLXJd0SEb8tup46vSjpI7Yvsj0k6VZJPy64prp5qQt4UNKhiPhW0fU0IiLujojx5c/7rZJ+2kWhruX/b75p+9Llb10n6bUCS+rujr2GhyQ9ZPtVSSVJt3VJ55iCb0salvT08l8dz0fEF4staW0RMW/7K5KelNQv6aGIOFhwWY24RtLnJb1ie//y9+6JiN0F1tRLbpf08HJT8IakfyqyGHaeAkBiUh6KAYCeRLADQGIIdgBIDMEOAIkh2AEgMQQ7ACSGYAeAxBDsAJCY/wfXSHEt1EzevgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116b05dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "X_mean = X- np.mean(X)\n",
    "Y_mean = Y- np.mean(Y)\n",
    "X_std= np.std(X)\n",
    "Y_std = np.std(Y)\n",
    "X_stdized = X_mean / X_std\n",
    "Y_stdized = Y_mean / Y_std\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(-7,7)\n",
    "plt.scatter(X_stdized, Y_stdized, s=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 The covariance\n",
    "The covariance for two variables $X_j$ and $X_k$:\n",
    "\n",
    "$s_{jk} = \\dfrac{\\sum_{i=1}^n (X_{ij}-\\bar X_{.j})(X_{ij}-\\bar X_{.k})}{n-1}= \\dfrac{\\sum_{i=1}^n x_{ij}x_{ik}}{n-1}$\n",
    "\n",
    "Denote $\\mathbf{S}$ the sample covariance matrix\n",
    "\n",
    "\\begin{equation} \\mathbf{S} = \\begin{bmatrix} s^2_{1} & s_{12} & \\dots & s_{1p} \\ s_{21} & s^2_{2} & \\dots & s_{2p} \\ \\vdots & \\vdots & \\ddots & \\vdots \\ s_{p1} & s_{p2} & \\dots & s^2_{p} \\end{bmatrix} \\end{equation}\n",
    "\n",
    "When you do the same computation with standardized variables, you get the correlation. Remember that the correlation $r_{jk}$ always lies between -1 and 1.\n",
    "\n",
    "$r_{jk} = \\dfrac{\\sum_{i=1}^n z_{ij}z_{ik}}{n-1}$\n",
    "\n",
    "Then, $\\mathbf{R}$ is the correlation matrix\n",
    "\n",
    "\\begin{equation} \\mathbf{R} = \\begin{bmatrix} 1 & r_{12} & \\dots & r_{1p} \\ r_{21} & 1 & \\dots & r_{2p} \\ \\vdots & \\vdots & \\ddots & \\vdots \\ r_{p1} & r_{p2} & \\dots & 1 \\end{bmatrix} \\end{equation}\n",
    "\n",
    "4. How does PCA work? Matrices and eigendecomposition\n",
    "4.1 Finding principal components\n",
    "$ \\mathbf{X}= (X_1, X_2, \\ldots, X_p)$ is a random variable. Then the principal components of $\\mathbf{X}$, denoted by $PC_1, \\ldots, PC_p$ satisfy these 3 conditions:\n",
    "\n",
    "$(PC_1, PC_2, \\ldots, PC_p)$ are mutually uncorrelated\n",
    "$var(PC_1)\\geq var(PC_2) \\geq \\ldots \\geq var(PC_p)$\n",
    "$PC_j = c_{j1} X_1+c_{j2} X_2+\\ldots+c_{jp} X_p$\n",
    "Note that for $j=1,\\ldots,p$ and $c_j = (c_{j1}, c_{j2}, \\ldots, c_{jp})$' is a vector of constants satisfying $ \\lVert{\\mathbf{c_j} \\rVert^2 = \\mathbf{c'j}\\mathbf{c_j}} = \\displaystyle\\sum^p{k=1} c^2_{kj}=1 $\n",
    "\n",
    "The variance of $PC$ is then: $var(PC_j) = var( c_{j1} X_1+c_{j2} X_2+\\ldots+c_{jp} X_p) \\ = c_{j1}^2 var(X_1) +c_{j2}^2 var(X_2) + \\ldots + c_{jp}^2 var(X_p) + 2 \\displaystyle\\sum_k\\sum_{l \\neq k}c_{jk}c_{jl} cov(X_k, X_l) \\ = c_j' \\Sigma c_j$\n",
    "\n",
    "In words, this means that variances can easily be computed using the coefficients used while making the linear combinations.\n",
    "\n",
    "We can prove that $var(PC_1)\\geq var(PC_2) \\geq \\ldots \\geq var(PC_p)$ is actually given by the eigenvalues $\\lambda_1\\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_3$ and the eigenvectors are given by $c_j = (c_{j1}, c_{j2}, \\ldots, c_{jp})$. From here on, we'll denote the eigenvectors by $e_j$ instead of $c_j$.\n",
    "\n",
    "Sources\n",
    "http://www.bbk.ac.uk/ems/faculty/brooms/teaching/SMDA/SMDA-02.pdf\n",
    "\n",
    "https://stackoverflow.com/questions/13224362/principal-component-analysis-pca-in-python\n",
    "\n",
    "https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
